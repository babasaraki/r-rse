# Correctness {#correct}

```{r correct-setup, include=FALSE}
source(here::here("_common.R"))
```

Now that we've written our software for counting and analyzing the words in classic texts,
how can we be sure that it’s producing reliable results?
The short is answer is that we can't be completely certain,
but that as in science,
we can test our expectations against reality to help us decide if we are sure enough.
In this chapter we explore the approaches available for testing code;
namely assertions, unit tests, integration tests and regression tests.

> **Scientist's Nightmare**
>
> If you need any motivation to learn how to test your code,
> look no further than the story of Geoffrey Chang [@Miller2006].
> A successful early career researcher in protein crystallography,
> Chang had to retract five published papers -- three from
> the prestigious journal Science -- because his code had
> inadvertently flipped two columns of data.

Our Zipf's Law project files are structured as they were at the end of the previous chapter:

```text
├── CONDUCT.md
├── LICENSE.md
├── Makefile
├── README.md
├── bin
│   ├── book_summary.sh
│   ├── collate.py
│   ├── countwords.py
│   ├── mymodule.py
│   ├── plotcounts.py
│   └── rcparams.yml
├── data
│   ├── README.md
│   ├── dracula.txt
│   ├── risk.txt
│   └── ...
└── results
    ├── dracula.csv
    ├── dracula.png
    └── ...
```

## Assertions {#assertions}

The first step toward getting the right answers from our programs
is to assume that mistakes will happen and to guard against them.
This is called defensive programming,
and the most common way to do it is to add assertions to our code
so that it checks itself as it runs.
An assertion is simply a statement that something must be true at a certain point in a program.
When Python sees an assertion, it evaluates the assertion’s condition.
If it’s true, Python does nothing, but if it’s false,
Python halts the program immediately and prints a user defined error message.

To demonstrate an assertion in action,
consider this code that halts as soon as the loop encounters a
weather station rainfall observation that isn’t positive:

```python
rainfall_observations = [1.5, 2.3, 0.7, -0.2, 4.4]
total = 0.0
for ob in rainfall_observations:
    assert ob >= 0.0, 'Rainfall observations should only contain positive values'
    total += ob
print('total rainfall is:', total)
```

```text
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-19-33d87ea29ae4> in <module>()
      2 total = 0.0
      3 for ob in rainfall_obs:
----> 4     assert ob > 0.0, 'Rainfall observations should only contain positive values'
      5     total += ob
      6 print('total rainfall is:', total)

AssertionError: Rainfall observations should only contain positive values
```

Programs like the Firefox browser are full of assertions:
10-20% of the code they contain are there to check that the other 80-90% are working correctly.
Broadly speaking, assertions fall into three categories:

- A *precondition* is something that must be true at the start of a function in order for it to work correctly.
- A *postcondition* is something that the function guarantees is true when it finishes.
- An *invariant* is something that is always true at a particular point inside a piece of code.

If the rainfall observation code above was part of a larger meteorological data processing script,
then that assertion is an example of an invariant -- at that particular point in the code,
it is always the case that a negative rainfall total makes no sense.
Data entry errors are possible, so it's important to check the validity of each observation.

Our `collate.py` script is a good example of the need for a precondition.
A user could easily overlook the doc string at the top of the script,
which states that it combines multiple word count CSV-files into a single cumulative count.
If they passed a series of text files instead of CSV files,
they'd get an error message that isn't particularly helpful in explaining the source of the problem.

```shell
$ python bin/collate.py data/jane_eyre.txt data/dracula.txt 
```
```text
Traceback (most recent call last):
  File "collate.py", line 29, in <module>
    main(args)
  File "collate.py", line 19, in main
    update_counts(reader, word_counts)
  File "collate.py", line 10, in update_counts
    for word, count in csv.reader(reader):
ValueError: too many values to unpack (expected 2)
```

A precondition (with informative error message) in the `main` function of `collate.py`
requiring the input files to end in ".csv" can clear up any potential confusion.

```python
def main(args):
    """Run the command line program."""
    word_counts = Counter()
    for file_name in args.infiles:
        assert file_name[-4:] == '.csv', "input files must be word count CSV files"
        with open(file_name, 'r') as reader:
            update_counts(reader, word_counts)
    mymodule.collection_to_csv(word_counts, num=args.num)
```

```shell
$ python bin/collate.py data/jane_eyre.txt data/dracula.txt 
```
```text
Traceback (most recent call last):
  File "collate.py", line 30, in <module>
    main(args)
  File "collate.py", line 18, in main
    assert file_name[-4:] == '.csv', "input files must be word count CSV files"
AssertionError: input files must be word count CSV files
```

## Unit testing {#unit-testing}

As the name suggests,
unit tests relate to small "units" or pieces of our code.
This typically means functions that we've defined.
What is considered to be the smallest code unit is subjective --
the body of a function can be long or short,
and shorter functions are arguably more unit-like than long ones --
but a good guideline is that if the code cannot be made any simpler
logically (you cannot split apart the addition operator) or 
practically (a function is self-contained and well defined),
then it is a unit.

In the case of our Zipf's Law software,
the `count_words` function in the `wordcounts.py` script is a good example of a code unit:

```
def count_words(reader):
    """Count the occurrence of each word in a string."""
    text = reader.read()
    findwords = re.compile(r"\w+", re.IGNORECASE)
    word_list = re.findall(findwords, text)
    word_counts = Counter(word_list)
    return word_counts
```

A single unit test will typically have:

-   a [fixture][fixture],
    which is the thing being tested (e.g., an array of numbers);
-   an [actual result][actual-result],
    which is what the code produces when given the fixture; and
-   an [expected result][expected-result]
    that the actual result is compared to.

In defining/creating an appropriate test fixture,
a common approach is to use a subset or smaller version of the data
that the function will typically process.
For instance, in order to write a unit test for the `count_words` function,
an appropriate fixture would be a small body of text
for which we know the frequency of each word.
The poem Risk by Anais Nin is a good choice,

```shell
$ cat data/risk.txt
```

```text
And then the day came,
when the risk
to remain tight
in a bud
was more painful
than the risk
it took
to blossom.
```

because it's so short that we can count the words by hand in order to create
an expected result variable:

```python
from collections import Counter

risk_poem_counts = {'the': 3, 'risk': 2, 'to': 2, 'And': 1, 'then': 1,
                    'day': 1, 'came': 1, 'when': 1, 'remain': 1, 'tight': 1,
                    'in': 1, 'a': 1, 'bud': 1, 'was': 1, 'more': 1,
                    'painful': 1, 'than': 1, 'it': 1, 'took': 1, 'blossom': 1}
expected_result = Counter(risk_poem_counts)
```

We can generate the actual result by calling the `word_counts` function,
and then use an assertion to check that the actual and expected results
are the same:

```python
import countwords

with open('data/risk.txt', 'r') as reader:
    actual_result = countwords.count_words(reader)
assert actual_result == expected_result
```

There's no output,
which means the test has passed
(recall that assertions only do something if the condition evaluates to false).

### Testing frameworks

Writing a single unit test for our Zipf's Law software is a good start,
but to be sure about the reliability of our code we'll probably want to write more unit tests.
To coordinate the running of many tests,
we can use a [test framework][test-framework] (also called a [test runner][test-runner]).
The most widely used test framework for Python is [`pytest`][pytest],
for which tests obey three rules:

1.  All tests are put in files whose names begin with `test_`.
2.  Each test is a function whose name also begins with `test_`.
3.  These functions use `assert` to check results.

Following these rules,
we can create a `test_zipfs.py` script that contains the test we just developed:

```python
from collections import Counter
import countwords

def test_word_count():
    """Test the counting of words.
    
    The example poem is Risk, by Anais Nin.
    """
    risk_poem_counts = {'the': 3, 'risk': 2, 'to': 2, 'And': 1, 'then': 1,
                        'day': 1, 'came': 1, 'when': 1, 'remain': 1, 'tight': 1,
                        'in': 1, 'a': 1, 'bud': 1, 'was': 1, 'more': 1,
                        'painful': 1, 'than': 1, 'it': 1, 'took': 1, 'blossom': 1}
    expected_result = Counter(risk_poem_counts)
    with open('risk.txt', 'r') as reader:
        actual_result = countwords.count_words(reader)
    assert actual_result == expected_result
```

The `pytest` library comes with a command-line tool that is also called `pytest`.
When we run it with no options,
it searches for all files in the working directory and subdirectories named `test_*.py`.
It runs the tests in these files and then summarizes their results.
(If you only want to run tests in a particular file,
use the command `pytest path/to/test_file.py`.)

```shell
$ pytest
```
```text
============================= test session starts ==============================
platform darwin -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.12.0
rootdir: /Users/amira
collected 1 item                                                               

bin/test_zipfs.py .                                                      [100%]

============================== 1 passed in 0.02s ===============================
```

In order to add to our suite of unit tests,
we can now simply add more `test_` functions to `test_zipfs.py`.
For instance,
besides the counting of words (which we've already tested),
the other critical part of our code is the calculation of the \(\alpha\) parameter.
Earlier we defined a power law relating \(\alpha\)
to the word frequency \(f\), word rank \(r\) 
and a constant \(c\) of proportionality (Section \@ref(git-advanced-theory)).\[
r = cf^{\frac{-1}{\alpha}}
\]
We also noted that Zipf's Law holds exactly when \(\alpha\) is equal to one.
Setting \(\alpha\) to one and re-arranging the power law gives the expression,\[
c = f/r
\]

We can use this expression generate synthetic word counts data 
(i.e. our test fixture) with a constant of proportionality
set to a hypothetical maximum word frequency of 600
(and thus \(r\) ranges from 1 to 600):

```python
import numpy as np

max_freq = 600
word_counts = np.floor(max_freq / np.arange(1, max_freq + 1)) 
```

(The `np.floor` function rounds down to the nearest whole number,
because you can't have fractional word counts.)

Passing this test fixture to the `get_power_law_params` function from our
`plotcounts.py` script,

```python
def get_power_law_params(word_counts):
    """
    Get the power law parameters.
    References
    ----------
    Moreno-Sanchez et al (2016) define alpha (Eq. 1),
      beta (Eq. 2) and the maximum likelihood estimation (mle)
      of beta (Eq. 6).
    Moreno-Sanchez I, Font-Clos F, Corral A (2016)
      Large-Scale Analysis of Zipf’s Law in English Texts.
      PLoS ONE 11(1): e0147073.
      https://doi.org/10.1371/journal.pone.0147073
    """
    mle = minimize_scalar(nlog_likelihood, bracket=(1 + 1e-10, 4),
                          args=(word_counts), method='brent')
    beta = mle.x
    alpha = 1 / (beta - 1)
    return alpha
```
should return an expected value of 1.0.

To test this, we can add a second test to `test_zipfs.py`,

```python
import numpy as np
from collections import Counter

import plotcounts
import countwords

def test_alpha():
    """Test the calculation of the alpha parameter.
    
    The test word counts satisfy the relationship,
      r = cf**(-1/alpha), where
      r is the rank,
      f the word count, and
      c is a constant of proportionality.

    To generate test word counts for an expected alpha value of 1.0,
      a maximum word frequency of 600 is used
      (i.e. c = 600 and r ranges from 1 to 600)
    """    
    max_freq = 600
    word_counts = np.floor(max_freq / np.arange(1, max_freq + 1))
    actual_alpha = plotcounts.get_power_law_params(word_counts)
    expected_alpha = 1.0
    assert actual_alpha == expected_alpha

def test_word_count():
    """Test the counting of words.
    
    The example poem is Risk, by Anais Nin.
    """
    risk_poem_counts = {'the': 3, 'risk': 2, 'to': 2, 'And': 1, 'then': 1,
                        'day': 1, 'came': 1, 'when': 1, 'remain': 1, 'tight': 1,
                        'in': 1, 'a': 1, 'bud': 1, 'was': 1, 'more': 1,
                        'painful': 1, 'than': 1, 'it': 1, 'took': 1, 'blossom': 1}
    expected_result = Counter(risk_poem_counts)
    with open('data/risk.txt', 'r') as reader:
        actual_result = countwords.count_words(reader)
    assert actual_result == expected_result
```

and then re-run pytest:

```shell
$ pytest
```

```text
============================= test session starts ==============================
platform darwin -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.12.0
rootdir: /Users/amira
collected 2 items                                                              

bin/test_zipfs.py F.                                                     [100%]

=================================== FAILURES ===================================
__________________________________ test_alpha __________________________________

    def test_alpha():
        """Test the calculation of the alpha parameter.
    
        The test word counts satisfy the relationship,
          r = cf**(-1/alpha), where
          r is the rank,
          f the word count, and
          c is a constant of proportionality.
    
        To generate test word counts for an expected alpha value of 1.0,
          a maximum word frequency of 600 is used
          (i.e. c = 600 and r ranges from 1 to 600)
        """
        max_freq = 600
        word_counts = np.floor(max_freq / np.arange(1, max_freq + 1))
        actual_alpha = plotcounts.get_power_law_params(word_counts)
        expected_alpha = 1.0
>       assert actual_alpha == expected_alpha
E       assert 0.9951524579316625 == 1.0

bin/test_zipfs.py:24: AssertionError
=========================== short test summary info ============================
FAILED bin/test_zipfs.py::test_alpha - assert 0.9951524579316625 == 1.0
========================= 1 failed, 1 passed in 0.85s ==========================
```

The output tells us that one test failed but the other test passed.
This is a nice feature of test runners like `pytest` -- they continue on and complete all the tests
rather than stopping at the first assertion failure like a regular Python script would.
Looking more closely at the output, we can see that although `test_alpha` failed,
the `actual_alpha` value of 0.9951524579316625 was awfully close to the expected value of 1.0.
In fact,
we don't expect the estimation routine used by `get_power_law_params` to be perfect -- it
is an estimation after all.
In order to edit `test_alpha` so that "close enough is good enough"
we can borrow a `pytest` function used for equating floating point values...  

### Testing and floating-point values {#correct-numeric}

Testing floating-point values is tricky because of the way computers represent these values.
Different operating systems, hardware,
and software have subtle differences in how these values are stored as numbers.
So, if we are testing a function that uses floating point numbers,
what do we compare its result to if computers represent the number differently?
If we try to test against a value with many decimal places (highly precise),
the test may fail because floating point values are represented as approximations.
No one has a good generic answer to this problem
because its root cause is that we're using approximations,
and each approximation has to be judged in its own context.

So what can you do to test your programs?
A good approach is to write a test that checks whether numbers are the same within some [tolerance][tolerance],
which is best expressed as a relative or absolute error.
The [absolute error][absolute-error] is the absolute value of
the difference between the approximation and the actual value.
The [relative error][relative-error] is the ratio of the absolute error to the value we're approximating.
For example,
if we are off by 1 in approximating 8+1 and 56+1,
we have the same absolute error,
but the relative error is larger in the first case than it is in the second.

For our `test_alpha` function,
we might decide that an absolute error of 0.01 in the estimation of \(\alpha\) is acceptable.
Using the `pytest` library,
we can define this tolerance level using `pytest.approx`:

```python
import pytest
import numpy as np
from collections import Counter

import plotcounts
import countwords

def test_alpha():
    """Test the calculation of the alpha parameter.
    
    The test word counts satisfy the relationship,
      r = cf**(-1/alpha), where
      r is the rank,
      f the word count, and
      c is a constant of proportionality.

    To generate test word counts for an expected alpha value of 1.0,
      a maximum word frequency of 600 is used
      (i.e. c = 600 and r ranges from 1 to 600)
    """    
    max_freq = 600
    word_counts = np.floor(max_freq / np.arange(1, max_freq + 1))
    actual_alpha = plotcounts.get_power_law_params(word_counts)
    expected_alpha = pytest.approx(1.0, abs=0.01)
    assert actual_alpha == expected_alpha

def test_word_count():
    """Test the counting of words.
    
    The example poem is Risk, by Anais Nin.
    """
    risk_poem_counts = {'the': 3, 'risk': 2, 'to': 2, 'And': 1, 'then': 1,
                        'day': 1, 'came': 1, 'when': 1, 'remain': 1, 'tight': 1,
                        'in': 1, 'a': 1, 'bud': 1, 'was': 1, 'more': 1,
                        'painful': 1, 'than': 1, 'it': 1, 'took': 1, 'blossom': 1}
    expected_result = Counter(risk_poem_counts)
    with open('data/risk.txt', 'r') as reader:
        actual_result = countwords.count_words(reader)
    assert actual_result == expected_result
```

Re-running pytest, all tests now pass:

```shell
$ pytest
```

```text
============================= test session starts ==============================
platform darwin -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.12.0
rootdir: /Users/amira
collected 2 items                                                              

bin/test_zipfs.py ..                                                     [100%]

============================== 2 passed in 0.69s ===============================
```

> **Testing Visualizations**
>
> One thing we haven't tested is the plot of our word count data.
> Testing visualizations is hard:
> any change to the dimension of the plot,
> however small,
> can change many pixels in a [raster image][raster-image],
> and cosmetic changes such as moving the legend up a couple of pixels
> will similarly cause false positives.
>
> The simplest solution is therefore to test the data used to produce the image
> (as we've done here)
> rather than the image itself.
> Unless we suspect that the plotting library contains bugs,
> the correct data should always produce the correct plot.

## Testing error handling {#correct-failure}

FIXME: explain how to test that expected errors occurred.

## Integration testing {#correct-integration}

We've seen in previous chapters that our Zipf's Law analysis is a two-step process.
The first involves counting the words in a text
and the second involves estimating the \(\alpha\) parameter from the word count.
Our unit tests `test_word_count` and `test_alpha` have checked that these two
individual components work in isolation, but do they work correctly together?
Checking that multiple units work correctly together is called *integration testing*.

The structure of integration tests is very similar to that of unit tests.
There is an expected result, which is compared against the actual result.
However, the work involved in creating the expected result
and/or setting up the code to run
can be considerably more complicated and involved. 
For example,
in the case of our Zipf's Law software an appropriate integration test fixture
might be a text file with a word frequency distribution that has a known alpha value. 
In order to create this text fixture,
we'll need a way of generating random words.
Fortunately, a Python library exits to do just that.
We can install it (and the `pypandoc` library it depends on) at the command line
using the Python Package Installer (pip): 

```shell
$ pip install pypandoc
$ pip install randomwordgenerator
```

Borrowing from the word count distribution we created for `test_alpha`,
we can then create a text file full of random words
with a frequency distribution that corresponds to an alpha value
of approximately 1.0.

```python
import numpy as np
from randomwordgenerator import randomwordgenerator

max_freq = 600
word_counts = np.floor(max_freq / np.arange(1, max_freq + 1))
random_words = randomwordgenerator.generate_random_words(n=max_freq)
fout = open('data/random_words.txt', 'w')
for index in range(max_freq):
    word_sequence = f"{random_words[index]} " * int(word_counts[index])
    fout.write(word_sequence + '\n')
fout.close()
```

The following integration test can then be added to `test_zipfs.py`,

```python
def test_integration():
    """Test the full word count to alpha parameter workflow."""    

    with open('data/random_words.txt', 'r') as reader:
        word_counts_dict = countwords.count_words(reader)
    word_counts_array = np.array(list(word_counts_dict.values()))
    actual_alpha = plotcounts.get_power_law_params(word_counts_array)
    expected_alpha = pytest.approx(1.0, abs=0.01)
    assert actual_alpha == expected_alpha
```

and we can re-run `pytest` to see whether the integration test passes.

```shell
$ pytest
```

```text
============================= test session starts ==============================
platform darwin -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.12.0
rootdir: /Users/amira
collected 3 items                                                                                         

bin/test_zipfs.py ...                                                    [100%]

=============================== 3 passed in 0.48s ==============================
```


## Regression testing {#correct-regression}

So far we've tested two simplified texts --
a short poem and a collection of random words
with a known frequency distribution -- but not a "real"  text.
The problem with testing a large text is that we don't know the expected result 
(e.g. it's not practical to sit and count the words in Dracula by hand).
For this kind of situation we can consider *regression testing*.
Rather than assuming that the test author knows what the expected result should be,
regression tests look to the past for the expected behavior.
The expected result is taken as what was previously computed for the same inputs.
In this way regression tests are great for letting developers know when and how a code base has changed,
but not great for letting anyone know why the change occurred.
The change between what a code produces now and what it computed before is called a regression.

In Section \@ref(git-advanced-zipf-verify)
we calculated an \(\alpha\) value of 1.1620041050803658 for Dracula.
We can use this expected result to add a regression test to `test_zipfs.py`:

```python
def test_regression():
    """Regression test for Dracula."""    

    with open('data/dracula.txt', 'r') as reader:
        word_counts_dict = countwords.count_words(reader)
    word_counts_array = np.array(list(word_counts_dict.values()))
    actual_alpha = plotcounts.get_power_law_params(word_counts_array)
    expected_alpha = pytest.approx(1.162, abs=0.001)
    assert actual_alpha == expected_alpha
```

```shell
$ pytest
```

```text
============================= test session starts ==============================
platform darwin -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.12.0
rootdir: /Users/amira
collected 4 items                                                                                

bin/test_zipfs.py ....                                                   [100%]

============================== 4 passed in 0.56s ===============================
```

## When to write tests

We've now met the three major types of test: unit, integration and regression.
At what point in the code development process should we be writing these tests?
The answer depends on who you ask.
Many programmers are passionate advocates of a practice called
[test-driven development][tdd] (TDD).
Rather than writing code and then writing tests,
they write the tests first and then write just enough code to make those tests pass.
Once the code is working,
they clean it up (Section \@ref(style-refactor)) and then move on to the next task.

TDD's advocates claim that working this way leads to better code in less time because:

1.  Writing tests clarifies what the code is actually supposed to do.

2.  It eliminates [confirmation bias][confirmation-bias].
    If someone has just written a function,
    they are predisposed to want it to be right,
    so they will bias their tests towards proving that it is correct
    instead of trying to uncover errors.

3.  Writing tests first ensures that they actually get written.

These arguments are plausible.
However,
studies such as @Fucc2016 don't support them:
in practice,
writing tests first or last doesn't appear to affect productivity.
What *does* have an impact is working in small, interleaved increments,
i.e.,
writing just a few lines of code and testing it before moving on
rather than writing several pages of code and then spending hours on testing.

So how do most data scientists figure out if their software is doing the right thing?
The answer is spot checks:
each time they produce an intermediate or final result,
they scan a table, create a chart, or inspect some summary statistics
to see if everything looks OK.
Their heuristics are usually easy to state,
like "there shouldn't be NAs at this point" or "the age range should be reasonable",
but applying those heuristics to a particular analysis always depends on
their evolving insight into the data in question.

By analogy with test-driven development,
we could call this process [checking-driven development][cdd] (CDD).
Each time we add a step to our pipeline and look at its output,
we can also add a check of some kind to the pipeline to ensure that
what we are checking for remains true as the pipeline evolves or is run on other data.
This helps reusability—it's amazing how often a one-off analysis
winds up being used many times—but the real goal is comprehensibility.
If someone can get our code and data,
then runs the code on the data,
and gets the same result that we did,
then our computation is reproducible,
but that doesn't mean they can understand it.
Comments help
(either in the code or as blocks of prose in a [computational notebook][computational-notebook]),
but they won't check that assumptions and invariants hold.
And unlike comments,
runnable assertions can't fall out of step with what the code is actually doing.

## Test coverage {#correct-coverage}

In defining tests for counting words and calculating the \(\alpha\) parameter,
it's likely that we have now tested (or "covered") all critical lines of our code.
To be sure,
we can use a tool to check the coverage of our tests.
Most Python programmers use the `coverage` library:

```shell
$ pip install coverage
```

Using the command-line utility that comes with the install,
we can then run `pytest` under coverage:

```shell
$ coverage run -m pytest
```

```text
====================================== test session starts =======================================
platform darwin -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.12.0
rootdir: /Users/amira
collected 4 items                                                                                

bin/test_zipfs.py ....                                                                     [100%]

======================================= 4 passed in 0.72s ========================================
```

This coverage command hasn't displayed any information of its own;
instead,
it puts coverage data in a file called `.coverage` (with a leading `.`) in the current directory.
To display that data,
we run:

```shell
$ coverage report -m
```

```text
Name            Stmts   Miss  Cover   Missing
---------------------------------------------
countwords.py      20      8    60%   19-21, 25-31
mymodule.py         7      4    43%   24-27
plotcounts.py      46     27    41%   41-47, 67-69, 74-90, 94-104
test_zipfs.py      32      0   100%
---------------------------------------------
TOTAL             105     39    63%
```

From this coverage summary,
we can see that not every single line of `countwords.py` or `plotcounts.py`
was executed when we ran the tests (only 60% and 41% of the lines were run, respectively).
This would seem to make sense, because much of the code in those scripts
is devoted to handling command line arguments or file input/output,
rather than the word counting and parameter estimation functionality 
that our unit, integration and regression tests focus on.
To make sure that's the case,
we can get a more complete report by running `coverage html` at the command line
and opening `htmlcov/index.html`.
Clicking on the name of our `countwords.py` script, for instance,
produces the colorized line-by-line display shown in Figure \@ref(fig:python-coverage).

```{r python-coverage, echo=FALSE, fig.cap="Coverage report"}
knitr::include_graphics("figures/py-rse/correct/python-coverage.png")
```

This output confirms that all lines relating to word counting were tested,
but not any of the lines related to the handling of command line arguments or
file input and output. 

### How much test coverage is enough? {#correct-enough}

How much we test our software depends on the purposes it will be used for.
If we are writing software for a safety-critical application such as a medical device,
we should aim for 100% code coverage,
i.e.,
every single line in the application should be tested.
In fact,
we should probably go further and aim for 100% [path coverage][path-coverage]
and ensure that every possible path through the code has been checked.

But most of us don't write software that people's lives depend on,
so requiring 100% code coverage is like asking for ten decimal places of accuracy
when checking the voltage of a household electrical outlet.
We always need to balance the effort required to create tests
against the likelihood that those tests will uncover useful information.

It's also important to understand that no amount of testing
can prove a piece of software is completely correct.
A function with only two numeric arguments has 2^128^ possible inputs.
Even if we could write the tests,
how could we be sure we were checking the result of each one correctly?

Testing data analysis pipelines is often harder than testing mainstream software applications,
since data analysts often don't know what the right answer is.
(If we did,
we would have submitted our report and moved on to the next problem already.)
The key distinction is the difference between [validation][validation],
which asks whether the specification is correct,
and [verification][verification],
which asks whether we have met that specification.
The difference between them is the difference between
building the right thing and building something right,
and the first is often hard for data scientists to address.

Luckily,
we can group the test cases for most functions into classes.
For example,
it might be possible to test a function that takes numbers as inputs
as well as needing to use only a few cases:
zero, a positive number, a negative one, and infinity.
If we want to go further,
we could check that it fails the right way when given a string or a list.
Similarly,
when testing a function that summarizes a table full of data,
we probably should check that it handles tables with:

-   no rows
-   only one row
-   many identical rows
-   rows having keys that are supposed to be unique, but aren't
-   rows that contain nothing but missing values

Some projects develop [checklists][checklist] to remind programmers what they ought to test.
These checklists can be a bit daunting for newcomers,
but they are a great way to pass on hard-earned experience.

## Continuous Integration {#correct-ci}

Now that we've defined our suite of tests,
we could simply run `pytest` manually every now and again to execute those tests and check our code. 
This is probably sufficient for relatively small/uncomplicated projects,
but a manual approach can be problematic.
If we make a bunch of different changes/commits to our code between runs of `pytest`,
it might be difficult to identify which change is responsible for a test failure.
The solution to this problem is [continuous integration][continuous-integration] (CI),
which involves running checks and tests automatically whenever a change is made.
This allows code developers to know immediately if new changes have caused problems,
making it much easier to fix the problem by seeing what was changed.
These tools can also be set up to run tests with several different configurations of the software
or on several different operating systems,
so that if (for example) a programmer makes a change on Windows 
and it then breaks only for Mac users or vice versa,
they can find out before releasing the software to others.

This section introduces a CI tool called [Travis CI][travis-ci],
which integrates well with [GitHub][github].
If Travis CI has been set up,
then every time a change is committed to a GitHub repository,
Travis CI creates a fresh environment,
makes a fresh clone of the repository
(which is described in Section \@ref(git-advanced-fork)),
and runs whatever commands the project's managers have set up.

### Creating a Travis account

To set up CI for your project, you first must:

1.  Create an account on [Travis CI][travis-ci].
2.  Link your Travis CI account to your GitHub account.
3.  Tell Travis CI to watch the repository that contains your project.

Creating an account with an online service may likely be a familiar process,
but linking your Travis CI account to your GitHub account may be something new.
Linking Travis to GitHub allows Travis to access all your GitHub repositories
in order to build and run the project.
Be careful when giving services access to your repository
and only trust well-established and -used services like Travis to have access.

Go through the process of creating an account. 
Next, tell Travis CI which repository you want it to watch
by clicking the "+" next to the "My Repositories" link
on the left-hand side of your Travis CI homepage (\@ref(fig:correct-add-repo)).

```{r correct-add-repo, echo=FALSE, fig.cap="Click to add a new GitHub repository to Travis CI"}
knitr::include_graphics("figures/py-rse/correct/travis-add-repo.png")
```

Add the GitHub repository we have been using throughout the course
(Section \@ref(git-cmdline-remotes))
by locating it in the repository list and
flicking the switch button so that it turns green
(Figure \@ref(fig:correct-list-repos)).
If the repository doesn't show up,
re-synchronize the list using the green "Sync account" button on the left sidebar.
If it still doesn't appear,
the repository likely belongs to someone else or is private.

```{r correct-list-repos, echo=FALSE, fig.cap="Find your Zipf's Law repository and switch it on"}
knitr::include_graphics("figures/py-rse/correct/travis-list-repos.png")
```

### Configuring a project for Travis

In order for Travis to know what to do with the project repository,
a file called `.travis.yml` must be present inside the repository.
So, let's create this file.

Make the `.travis.yml` file in the root directory of the repository.
(The leading `.` in the name hides the file from casual listings on Mac or Linux,
but not on Windows.)
This file controls Travis CI's operation,
and is written in the [YAML][yaml] format we met earlier (Chapter \@ref(configuration)).

Now that we have a `.travis.yml` file, open it and add the following lines:

```yaml
language: python

python:
- "3.6"

script:
- pytest
```

The `language` key tells Travis CI which programming language to use,
so that it knows which of its standard [virtual machines][virtual-machine] to run
as a starting point for the project.
The `python` key specifies the version/s of Python to use, 
while the `script` key lists the commands to run. 
In this case, we want to run `pytest` in order to run our tests.
We can now go ahead and push the `.travis.yml` file to GitHub.

```shell
$ git add .travis.yml 
$ git commit -m "Initial commit of travis configuration file"
```
```text
[master 71084f7] Initial commit of travis file
 1 file changed, 4 insertions(+)
 create mode 100644 .travis.yml
```
```shell
$ git push origin master
```
```text
Enumerating objects: 4, done.
Counting objects: 100% (4/4), done.
Delta compression using up to 4 threads
Compressing objects: 100% (2/2), done.
Writing objects: 100% (3/3), 344 bytes | 344.00 KiB/s, done.
Total 3 (delta 0), reused 0 (delta 0)
To https://github.com/amira-khan/zipf.git
   1f0590b..71084f7  master -> master
```

In response to this commit,
Travis CI follows our instructions from the `.travis.yml` file
and reports a summary on whether the build passed,
shown in green,
or whether it produced warnings or errors,
shown in red.
In this case, we can see that the build failed (Figure \@ref(fig:correct-build-fail)).

```{r correct-build-fail, echo=FALSE, fig.cap="Travis Build Overview"}
knitr::include_graphics("figures/py-rse/correct/travis-build-fail.png")
```

Scrolling down to read the job log in detail,
it says that it "could not locate requirements.txt."
This is an issue because the Python scripts that are run when `pytest` is executed
(i.e. `test_zipfs.py`, `plotcounts.py`, `countwords.py` and `mymodule.py`)
import a number of packages that don't come with the
[Python Standard Library][python-standard-library].
To address this issue, 
we need to add an `install` key to `.travis.yml`: 

```yaml
language: python

python:
- "3.6"

install:
- pip install -r requirements.txt

script:
- pytest
```

In a new `requirements.txt` file,
we list the libraries that need to be installed:

```text
numpy
pandas
matplotlib
scipy
pytest
pyyaml
```

When we commit these changes to GitHub,

```shell
$ git add .travis.yml
$ git add requirements.txt 
$ git commit -m "Adding requirements"
```
```text
[master d96593f] Adding requirements
 2 files changed, 16 insertions(+), 1 deletion(-)
 create mode 100644 requirements.txt
```
```shell
$ git push origin master
```
```text
Enumerating objects: 4, done.
Counting objects: 100% (4/4), done.
Delta compression using up to 4 threads
Compressing objects: 100% (2/2), done.
Writing objects: 100% (3/3), 344 bytes | 344.00 KiB/s, done.
Total 3 (delta 0), reused 0 (delta 0)
To https://github.com/amira-khan/zipf.git
   1f0590b..71084f7  master -> master
```

Travis CI automatically runs again and this time the build completes successfully
and our tests pass (Figure \@ref(fig:correct-build-pass)).

```{r correct-build-pass, echo=FALSE, fig.cap="Travis Build Overview"}
knitr::include_graphics("figures/py-rse/correct/travis-build-pass.png")
```

To summarize what Travis has done, it:

1.  Created a new Linux virtual machine.
1.  Installed the desired version of Python.
1.  Installed the software described in `requirements.txt`.
1.  Ran the commands below the `script` key.
1.  Reported the results at <code>https://travis-ci.org/<em>user</em>/<em>repo</em></code>,
    where <code><em>user/repo</em></code>
    identifies the repository.


## Exercises {#correct-exercises}

TODO: Need more exercises.

### Testing Assertions

Given a sequence of a number of cars, the function `get_total_cars` returns
the total number of cars.

```python
get_total_cars([1, 2, 3, 4])
```
```text
10
```
```python
get_total_cars(['a', 'b', 'c'])
```
```text
ValueError: invalid literal for int() with base 10: 'a'
```
Explain in words what the assertions in this function check,
and for each one,
give an example of input that will make that assertion fail.

```python
def get_total(values):
    assert len(values) > 0
    for element in values:
        assert int(element)
    values = [int(element) for element in values]
    total = sum(values)
    assert total > 0
    return total
```

> **Solution**
> * The first assertion checks that the input sequence `values` is not empty.
>   An empty sequence such as `[]` will make it fail.
> * The second assertion checks that each value in the list can be turned into an integer.
>   Input such as `[1, 2,'c', 3]` will make it fail.
> * The third assertion checks that the total of the list is greater than 0.
>   Input such as `[-10, 2, 3]` will make it fail.


### Add the Travis CI status to your README

You'll notice that the README file in many GitHub repositories includes a little
Travis CI display status logo.
Follow [these instructions](https://docs.travis-ci.com/user/status-images/) to include
the status display in the REAMDE for this Zipf's Law project.


## Key Points {#correct-keypoints}

```{r, child="keypoints/py-rse/correct.md"}
```

```{r, child="./links.md"}
```
